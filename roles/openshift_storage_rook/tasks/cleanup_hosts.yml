---
- name: Get Rook Ceph storage devices state
  command: "pvdisplay -C --noheadings -o pv_name,vg_name {% for device in hostvars[item].ceph_devices %}{{ device }} {% endfor %}"
  register: devices_info
  delegate_to: "{{ item }}"
  with_items: "{{ rook_ceph_nodes | default([]) }}"
  failed_when: False
  when: rook_ceph_wipe

  # Runs "lvremove -ff <vg>; vgremove -fy <vg>; pvremove -fy <pv>" for every device found to be a physical volume.
- name: Clear GlusterFS storage device contents
  shell: "{% for line in item.stdout_lines %}{% set fields = line.split() %}{% if fields | count > 1 %}lvremove -ff {{ fields[1] }}; vgchange -an {{ fields[1] }}; vgremove -fy {{ fields[1] }}; dmsetup remove {{ fields[1] }}; {% endif %}pvremove -fy {{ fields[0] }}; {% endfor %}"
  delegate_to: "{{ item.item }}"
  with_items: "{{ devices_info.results }}"
  register: clear_devices
  until:
  - "'contains a filesystem in use' not in clear_devices.stderr"
  delay: 1
  retries: 30
  when:
  - rook_ceph_wipe
  - item.stdout_lines | count > 0

- name: Zap the disks to fresh usable state
  command: "sgdisk --zap-all {% for device in hostvars[item].ceph_devices %}{{ device }} {% endfor %} && ls /dev/mapper/ceph-* | xargs -I% -- dmsetup remove % && rm -rf /dev/ceph-*"
  delegate_to: "{{ item }}"
  with_items: "{{ rook_ceph_nodes | default([]) }}"
  failed_when: False
  when: rook_ceph_wipe

- name: Wipe filesystem signatures from storage devices
  command: "wipefs -a {% for device in hostvars[item].ceph_devices %}{{ device }} {% endfor %}"
  delegate_to: "{{ item }}"
  with_items: "{{ rook_ceph_nodes | default([]) }}"
  failed_when: False
  when: rook_ceph_wipe
- name: Delete ceph directories
  file:
    path: "{{ item[1] }}"
    state: absent
  delegate_to: "{{ item[0] }}"
  with_nested:
  - "{{ rook_ceph_nodes | default([]) }}"
  - /var/lib/rook
  - /etc/ceph
- name: Delete ceph directories
  file:
    path: "{{ item[1] }}"
    state: absent
  delegate_to: "{{ item[0] }}"
  with_nested:
  - "{{ nodes | default([]) }}"
  - /etc/ceph